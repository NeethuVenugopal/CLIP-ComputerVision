{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNgrqR5f3ZXSum57UxYitZb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NeethuVenugopal/CLIP-ComputerVision/blob/main/clip_image.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Inference using CLIP Pretrained model"
      ],
      "metadata": {
        "id": "g3E67l0BayZP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --yes -c pytorch pytorch=1.7.1 torchvision cudatoolkit=11.0\n",
        "!pip install ftfy regex tqdm\n",
        "!pip install git+https://github.com/openai/CLIP.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9FNxHzV5lCTo",
        "outputId": "f1efc978-ab0e-4043-a03f-0e47d47f5531"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Usage:   \n",
            "  pip3 install [options] <requirement specifier> [package-index-options] ...\n",
            "  pip3 install [options] -r <requirements file> [package-index-options] ...\n",
            "  pip3 install [options] [-e] <vcs project url> ...\n",
            "  pip3 install [options] [-e] <local project path> ...\n",
            "  pip3 install [options] <archive url/path> ...\n",
            "\n",
            "no such option: --yes\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.1.3-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.4/53.4 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy) (0.2.12)\n",
            "Installing collected packages: ftfy\n",
            "Successfully installed ftfy-6.1.3\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-7jufk5ak\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-7jufk5ak\n",
            "  Resolved https://github.com/openai/CLIP.git to commit a1d071733d7111c9c014f024669f959182114e33\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (6.1.3)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (4.66.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2.1.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (0.16.0+cu121)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy->clip==1.0) (0.2.12)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->clip==1.0) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->clip==1.0) (1.3.0)\n",
            "Building wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369497 sha256=918898e1f2fc007ae558baa2aeeec08778e0dfa7de3c0ab1ae2263c302c7ff44\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-53xp2yzu/wheels/da/2b/4c/d6691fa9597aac8bb85d2ac13b112deb897d5b50f5ad9a37e4\n",
            "Successfully built clip\n",
            "Installing collected packages: clip\n",
            "Successfully installed clip-1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TwE2KRvMawhK",
        "outputId": "91646ec0-6eee-402b-ab42-bd28cf63b2a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|███████████████████████████████████████| 338M/338M [00:04<00:00, 82.7MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label probs: [[0.005516 0.007195 0.9873  ]]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import clip\n",
        "from PIL import Image\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "\n",
        "image = preprocess(Image.open(\"cat.jpg\")).unsqueeze(0).to(device)\n",
        "text = clip.tokenize([\"a diagram\", \"a dog\", \"a cat\"]).to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    image_features = model.encode_image(image)\n",
        "    text_features = model.encode_text(text)\n",
        "\n",
        "    logits_per_image, logits_per_text = model(image, text)\n",
        "    probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
        "\n",
        "print(\"Label probs:\", probs)  # prints: [[0.9927937  0.00421068 0.00299572]]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import clip\n",
        "import torch\n",
        "from torchvision.datasets import CIFAR100\n",
        "\n",
        "# Load the model\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load('ViT-B/32', device)\n",
        "\n",
        "# Download the dataset\n",
        "cifar100 = CIFAR100(root=os.path.expanduser(\"~/.cache\"), download=True, train=False)\n",
        "\n",
        "# Prepare the inputs\n",
        "image, class_id = cifar100[3637]\n",
        "image_input = preprocess(image).unsqueeze(0).to(device)\n",
        "text_inputs = torch.cat([clip.tokenize(f\"a photo of a {c}\") for c in cifar100.classes]).to(device)\n",
        "\n",
        "# Calculate features\n",
        "with torch.no_grad():\n",
        "    image_features = model.encode_image(image_input)\n",
        "    text_features = model.encode_text(text_inputs)\n",
        "\n",
        "# Pick the top 5 most similar labels for the image\n",
        "image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
        "values, indices = similarity[0].topk(5)\n",
        "\n",
        "# Print the result\n",
        "print(\"\\nTop predictions:\\n\")\n",
        "for value, index in zip(values, indices):\n",
        "    print(f\"{cifar100.classes[index]:>16s}: {100 * value.item():.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DFeU80a-eriY",
        "outputId": "d5be90f2-b9b1-49fd-cc05-1f83cb45082e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to /root/.cache/cifar-100-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 169001437/169001437 [00:03<00:00, 48837466.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /root/.cache/cifar-100-python.tar.gz to /root/.cache\n",
            "\n",
            "Top predictions:\n",
            "\n",
            "           snake: 65.48%\n",
            "          turtle: 12.30%\n",
            "    sweet_pepper: 3.87%\n",
            "          lizard: 1.89%\n",
            "       crocodile: 1.72%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import IPython.display as display\n",
        "display.display(image)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        },
        "id": "RGgkZLBcmZq6",
        "outputId": "0056769e-4e19-41b7-dcfe-6e5355748b1f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=32x32>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import clip\n",
        "import torch\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import CIFAR100\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load the model\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load('ViT-B/32', device)\n",
        "\n",
        "# Load the dataset\n",
        "root = os.path.expanduser(\"~/.cache\")\n",
        "train = CIFAR100(root, download=True, train=True, transform=preprocess)\n",
        "test = CIFAR100(root, download=True, train=False, transform=preprocess)\n",
        "\n",
        "\n",
        "def get_features(dataset):\n",
        "    all_features = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(DataLoader(dataset, batch_size=100)):\n",
        "            features = model.encode_image(images.to(device))\n",
        "\n",
        "            all_features.append(features)\n",
        "            all_labels.append(labels)\n",
        "\n",
        "    return torch.cat(all_features).cpu().numpy(), torch.cat(all_labels).cpu().numpy()\n",
        "\n",
        "# Calculate the image features\n",
        "train_features, train_labels = get_features(train)\n",
        "test_features, test_labels = get_features(test)\n",
        "\n",
        "# Perform logistic regression\n",
        "classifier = LogisticRegression(random_state=0, C=0.316, max_iter=1000, verbose=1)\n",
        "classifier.fit(train_features, train_labels)\n",
        "\n",
        "# Evaluate using the logistic regression classifier\n",
        "predictions = classifier.predict(test_features)\n",
        "accuracy = np.mean((test_labels == predictions).astype(float)) * 100.\n",
        "print(f\"Accuracy = {accuracy:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45Hp-pmgmi_2",
        "outputId": "84600b78-6b25-44f3-c43d-1046685ac731"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 500/500 [02:00<00:00,  4.15it/s]\n",
            "100%|██████████| 100/100 [00:23<00:00,  4.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy = 79.950\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WMn2hLpNnbLK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}